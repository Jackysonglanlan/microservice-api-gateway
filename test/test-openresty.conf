
pid ../logs/pids/openresty.pid;

# run 'nproc' to determine the number of worker_processes
worker_processes  1;

# [linux-only] 为每个进程分配cpu，每个核分配一个
# worker_cpu_affinity 0001 0010 0100 1000;

# 一个nginx 进程打开的最多文件描述符数目
# 理论值应该是最多打开文件数（ulimit -n）与nginx 进程数相除，但是nginx 分配请求并不是那么均匀，所以最好与ulimit -n 的值保持一致
# 比如: 填写10240, 8个进程, 总并发量达到3-4万时就有进程可能超过10240了，这时会返回502错误。
worker_rlimit_nofile 65535;

daemon off;

events{
  # 每个进程允许的最多连接数, 理论上每台nginx 服务器的最大连接数为 worker_processes * worker_connections
  #
  # 在设置了反向代理的情况下，max_clients= worker_processes * worker_connections / 4
  # 为什么上面反向代理要除以4，应该说是一个经验值
  #
  # 根据以上条件，正常情况下的Nginx Server可以应付的最大连接数为：4 * 8000 = 32000
  # worker_connections 值的设置跟物理内存大小有关
  # 因为并发受IO约束，max_clients的值须小于系统可以打开的最大文件数
  # 而系统可以打开的最大文件数和内存大小成正比，一般1GB内存的机器上可以打开的文件数大约是10万左右
  #
  # 我们来看看360M内存的VPS可以打开的文件句柄数是多少：
  # $ cat /proc/sys/fs/file-max
  # 输出 34336
  #
  # 32000 < 34336，即并发连接总数小于系统可以打开的文件句柄总数，这样就在操作系统可以承受的范围之内
  # 所以，worker_connections 的值需根据 worker_processes 进程数目和系统可以打开的最大文件总数进行适当地进行设置
  # 使得并发总数小于操作系统可以打开的最大文件数目
  # 其实质也就是根据主机的物理CPU和内存进行配置
  # 当然，理论上的并发总数可能会和实际有所偏差，因为主机还有其他的工作进程需要消耗系统资源。
  worker_connections 65535;

  # 与apache相类，nginx针对不同的操作系统，有不同的事件模型
  #       A）标准事件模型
  #        Select、poll属于标准事件模型，如果当前系统不存在更有效的方法，nginx会选择select或poll
  #       B）高效事件模型
  # Kqueue：使用于 FreeBSD 4.1+, OpenBSD 2.9+, NetBSD 2.0 和 MacOS X. 使用双处理器的MacOS X系统使用kqueue可能会造成内核崩溃。
  # Epoll: 使用于Linux内核2.6版本及以后的系统。
  # use epoll;

  # 收到一个新连接通知后接受尽可能多的连接
  multi_accept on;

  accept_mutex off;
}

http {

  ####### --- security --- #######

  # 并不会让nginx执行的速度更快，但它可以关闭在错误页面中的nginx版本数字，这样对于安全性是有好处的
  server_tokens off;

  more_clear_headers Server;
  proxy_hide_header X-Powered-By;

  ####### --- normal --- #######

  # 头文件中的默认的字符集
  charset utf-8;

  # 可以让sendfile()发挥作用。sendfile()可以在磁盘和TCP socket之间互相拷贝数据(或任意两个文件描述符)
  # Pre-sendfile是传送数据之前在用户空间申请数据缓冲区。之后用read()将数据从文件拷贝到这个缓冲区，write()将缓冲区数据写入网络
  # sendfile()是立即将数据从磁盘读到OS缓存。因为这种拷贝是在内核完成的，sendfile()要比组合read()和write()以及
  # 打开关闭丢弃缓冲更加有效
  sendfile on;

  # 减少阻塞调用sendfile()所花费的最长时间，因为NGINX不会尝试一次将整个文件发送出去，而是每次发送大小为512KB的块数据
  sendfile_max_chunk 512k;

  # 在一个数据包里发送所有头文件，而不一个接一个的发送
  tcp_nopush on;

  # 不要缓存数据，而是一段一段的发送--当需要及时发送数据时，就应该给应用设置这个属性，这样发送一小块数据信息时就不能立即得到返回值
  tcp_nodelay on;

  # 给客户端分配keep-alive链接超时时间。服务器将在这个超时时间过后关闭链接。我们将它设置低些可以让ngnix持续工作的时间更长
  keepalive_timeout 10;

  # 关闭不响应的客户端连接。这将会释放那个客户端所占有的内存空间
  reset_timedout_connection on;

  # 指定客户端的响应超时时间。这个设置不会用于整个转发器，而是在两次客户端读取操作之间
  # 如果在这段时间内，客户端没有读取任何数据，nginx就会关闭连接。
  send_timeout 30;

  # 设置用于保存各种key（比如当前连接数）的共享内存的参数。5m就是5兆字节，这个值应该被设置的足够大
  # 以存储（32K*5）32byte状态或者（16K*5）64byte状态
  limit_conn_zone $binary_remote_addr zone=addr:50m;

  # 为给定的key设置最大连接数。这里key是addr，我们设置的值是100，也就是说我们允许每一个IP地址最多同时打开有100个连接
  limit_conn addr 100;

  # 保存服务器名字的hash表是由指令 server_names_hash_max_size 和 server_names_hash_bucket_size 所控制的
  # 参数 hash_bucket_size 总是等于hash表的大小，并且是一路处理器缓存大小的倍数
  # 在减少了在内存中的存取次数后，使在处理器中加速查找hash表键值成为可能
  # 如果 hash_bucket_size 等于一路处理器缓存的大小，那么在查找键的时候，最坏的情况下在内存中查找的次数为2
  # 第一次是确定存储单元的地址，第二次是在存储单元中查找键值
  # 因此，如果Nginx给出需要增大 hash_max_size 或 hash_bucket_size 的提示，那么首要的是增大前一个参数的大小.
  server_names_hash_bucket_size 128;

  ####### --- open_file --- #######

  # 这个将为打开文件指定缓存，默认是没有启用的，max 指定缓存数量
  # 建议和打开文件数一致，inactive 是指经过多长时间文件没被请求后删除缓存
  open_file_cache max=65535 inactive=60s;

  # 多长时间检查一次缓存的有效信息
  open_file_cache_valid 30s;

  # open_file_cache 指令中的inactive 参数时间内文件的最少使用次数，如果超过这个数字，文件描述符一直是在缓存中打开的
  # 设置为1: 如果有一个文件在inactive 时间内一次没被使用，它将被移除
  open_file_cache_min_uses 1;

  ####### --- client --- #######

  # 该值必须设置为“系统分页大小”的整倍数。
  client_header_buffer_size 4k;
  client_header_timeout 10;
  client_body_timeout 10;

  # 如果设置为比较大的数值，例如256k，那么，无论使用firefox还是IE浏览器，来提交任意小于256k的图片，都很正常。
  # 如果注释该指令，使用默认的client_body_buffer_size设置，也就是操作系统页面大小的两倍，8k或者16k，问题就出现了:
  # 无论使用firefox4.0还是IE8.0，提交一个比较大，200k左右的图片，都返回500 Internal Server Error错误
  client_body_buffer_size 512k;

  ####### --- proxy --- #######

  # 后端服务器连接的超时时间_发起握手等候响应超时时
  proxy_connect_timeout 90;

  # 连接成功后_等候后端服务器响应时间_其实已经进入后端的排队之中等候处理（也可以说是后端服务器处理请求的时间）
  proxy_read_timeout 180;

  # 后端服务器数据回传时间_就是在规定时间之内后端服务器必须传完所有的数据
  proxy_send_timeout 180;

  # 设置从被代理服务器读取的第一部分应答的缓冲区大小，通常情况下这部分应答中包含一个小的应答头
  # 默认情况下这个值的大小为指令proxy_buffers中指定的一个缓冲区的大小，不过可以将其设置为更小
  proxy_buffer_size 256k;

  # 设置用于读取应答（来自被代理服务器）的缓冲区数目和大小，默认情况也为分页大小，根据操作系统的不同可能是4k或者8k
  proxy_buffers 4 256k;

  proxy_busy_buffers_size 256k;

  # 设置在写入proxy_temp_path时数据的大小，预防一个工作进程在传递文件时阻塞太长
  proxy_temp_file_write_size 256k;

  # 重写 Connection 头，便于缓存和 upstream 的 keepalive
  # proxy_set_header Connection "";

  # ---- proxy cache ---- #
  #
  # WARN: nginx 的缓存是存在磁盘上的 (对比转发请求到后端服务器，已经足够快了，不过仍然是磁盘 IO)
  #
  # proxy_cache_path  ../tmp/nginx-cache levels=1:2 keys_zone=cache:8m max_size=3000m inactive=600m;
  # proxy_temp_path ../tmp/nginx-cache-tmp;
  # proxy_cache cache;
  # # 对于 200 响应的数据，缓存 1s
  # proxy_cache_valid 200 1s;
  # proxy_cache_valid 404 1s;
  # proxy_cache_key yqj$request_uri$scheme;
  # # 限制填充缓存的并发尝试数量，这样当一条缓存入口被创建后，对该资源的请求将会在 NGINX 中排队
  # proxy_cache_lock on;
  # # 配置 NGINX，使它提供旧的（最近缓存的）内容，同时更新缓存入口
  # proxy_cache_use_stale updating;
  # # 查看缓存命中率(仅用于测试，正式环境勿开)
  # add_header  Nginx-Cache "$upstream_cache_status";

  proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;

  ####### --- gzip --- #######

  gzip on;
  gzip_disable "MSIE [1-6]\.(?!.*SV1)";

  # 在压缩资源之前，先查找是否有预先gzip处理过的资源。这要求你预先压缩你的文件
  # 从而允许你使用最高压缩比，这样nginx就不用再压缩这些文件了
  # gzip_static on;

  # gzip_types设置对 gzip_static 无效
  gzip_types
    application/x-font-ttf application/x-javascript application/javascript application/json
    text/css text/javascript text/json text/plain
    image/svg+xml image/jpeg image/gif image/png;

  gzip_http_version 1.1;

  # Tell proxies to cache both the gzipped and regular version of a resource
  # whenever the client's Accept-Encoding capabilities header varies;
  # Avoids the issue where a non-gzip capable client (which is extremely rare
  # today) would display gibberish if their proxy gave them the gzipped version.
  gzip_vary          on;

  # Compression level (1-9).
  # 5 is a perfect compromise between size and cpu usage, offering about
  # 75% reduction for most ascii files (almost identical to level 9).
  gzip_comp_level  5;

  # Don't compress anything that's already small and unlikely to shrink much
  # if at all (the default is 20 bytes, which is bad as that usually leads to
  # larger files after gzipping).
  gzip_min_length  2k;

  # Compress data even for clients that are connecting to us via proxies,
  # identified by the "Via" header
  gzip_proxied     expired no-cache no-store private auth;

  # 设置系统获取几个单位的缓存用于存储gzip的压缩结果数据流。4 16k 代表以 16k 为单位，按照原始数据大小以 16k 为单位的4倍申请内存
  gzip_buffers 16 8k;

  ####### --- log --- #######

  log_format  access  '$remote_addr - $remote_user [$time_local] "$request" '
    '$status $body_bytes_sent "$http_referer" '
    '"$http_user_agent" $http_x_forwarded_for';

  access_log logs/nginx/access.log access;
  error_log logs/nginx/error.log error;

  ####### --- lua --- #######

  # lua require() 的装载路径, 注意 libs 下的子目录需要通过 '.' 来访问
  # 比如:
  # require('foo.bar')，openresty 会去寻找 $prefix/lua/libs/foo/bar.lua
  lua_package_path "$prefix/../lua/libs/?.lua;$prefix/../lua/src/?.lua;;";
  lua_package_cpath "$prefix/../lua/libs/?.so;;";

  # see https://github.com/thibaultcha/lua-resty-mlcache
  # TODO: 不知道这个值是所有 nginx work 共享，还是每个 work 独享，这个关系到 pm2 的 ‘max_memory_restart’ 配置
  lua_shared_dict yqj_global_cache_dict 400m;

  ################
  #
  # Openresty 根据 nginx 处理流程提供的拦截器 (每个拦截器都有 by_lua(deprecated) / by_lua_file / by_lua_block 3种):
  #
  # init_by_lua: lua 环境初始化
  #
  # --------- 下面是基于每个 请求-响应 处理过程的拦截器 ---------
  #
  # set_by_lua: 流程分支处理判断, 变量初始化
  #
  # rewrite_by_lua: 转发、重定向、缓存等功能(例如特定请求代理到外网)
  #
  # access_by_lua: IP 准入、接口权限等情况集中处理(例如配合 iptable 完成简单防火墙)
  #
  # content_by_lua: 内容生成
  #
  # header_filter_by_lua: 应答 HTTP 过滤处理(例如添加头部信息)
  #
  # body_filter_by_lua: 应答 BODY 过滤处理(例如完成应答内容统一成大写)
  #
  # log_by_lua: 会话完成后本地异步完成日志记录(日志可以记录在本地，还可以同步到其他机器)
  #
  # 更多内容见: https://moonbingbing.gitbooks.io/openresty-best-practices/content/ngx_lua/phase.html
  #
  ################

  # 为啥要动用 init_by_lua, 见 https://github.com/openresty/lua-nginx-module#init_by_lua
  init_by_lua_file ../lua/src/init/init.lua;

  ####### --- vhosts --- #######

  include vhosts/current/*.conf;

} # end http
